{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reviews Analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HM4fkD6VDkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-eu.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz \n",
        "!tar xvf spark-2.4.3-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnb_ePUyQIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P",
        "colab_type": "text"
      },
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dVriRyIfdpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc4ZxWeYkwh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark import HiveContext,SQLContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXwPCLtpk9-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"10g\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyl8QWj7V-2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg5jxBLgqKzh",
        "colab_type": "text"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et3S0-F3qKzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ubfxlaqKzl",
        "colab_type": "text"
      },
      "source": [
        "### Creating Spark and Hive Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5NOGysGqKzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import HiveContext\n",
        "\n",
        "sqlContext = HiveContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nU9LXWKqKzp",
        "colab_type": "text"
      },
      "source": [
        "### Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbWgRT3hqKzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_df =sqlContext.read.format('com.databricks.spark.csv')\\\n",
        ".options(header='true',inferschema='true')\\\n",
        ".load(\"reviews_data_cleaned.csv\")\n",
        "\n",
        "review_df = review_df.drop(\"date\",\"listing_id\",\"reviewer_id\",\"reviewer_name\",\"id\",\"_c0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HzZRO7b5qKzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b29a9bdc-0bb1-4f03-cfc8-f8b15c77e903"
      },
      "source": [
        "review_df.show(4)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            comments|\n",
            "+--------------------+\n",
            "|         Great host |\n",
            "|Nice room for the...|\n",
            "|Very nice apt.  N...|\n",
            "|Great place to st...|\n",
            "+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beZH4DleqKzx",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNMHsPjIqKzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = review_df.randomSplit([0.5, 0.5])\n",
        "reviews_df = splits[0]\n",
        "reviews_df_test = splits[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu518GEIqKz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "643f9411-c181-4178-acd0-bd1a5982c28e"
      },
      "source": [
        "reviews_df.printSchema()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- comments: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RcM2494qKz4",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYkHAb1FqKz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Removing comments with only . (dots)\n",
        "reviews_df = reviews_df.withColumn('comments',regexp_replace(reviews_df['comments'],'[\\.,)]',' '))\n",
        "\n",
        "# Dropping NA from Train and Test\n",
        "reviews_df = reviews_df.dropna()\n",
        "reviews_df_test = reviews_df_test.dropna()\n",
        "\n",
        "\n",
        "# Function to Drop NAN and NULLS and Clean String\n",
        "from pyspark.sql.functions import col, isnan, when, trim\n",
        "def to_null(c):\n",
        "    return when(~(col(c).isNull() | col(c).isin([\" \"]) |col(c).isin([\"nan\"]) | (trim(col(c)) == \"\")), col(c))\n",
        "\n",
        "\n",
        "# Applying to_null Function\n",
        "reviews_df=reviews_df.select([to_null(c).alias(c) for c in reviews_df.columns]).na.drop()\n",
        "reviews_df_test = reviews_df_test.select([to_null(c).alias(c) for c in reviews_df_test.columns]).na.drop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WU0YHqUqKz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6902a6a4-fed8-4edb-8cbd-3e1f21761bff"
      },
      "source": [
        "reviews_df.count()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxSD15zRqKz-",
        "colab_type": "text"
      },
      "source": [
        "### Generating Text Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8D9uNwmqKz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from textblob import TextBlob\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "# Function to get sentiment score\n",
        "def sentiment_analyzer(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "sentiment_analyzer_udf = udf(sentiment_analyzer, FloatType())\n",
        "\n",
        "\n",
        "# Generating Score\n",
        "df = reviews_df.withColumn(\"sentiment_score\", sentiment_analyzer_udf(reviews_df[\"comments\"]))\n",
        "df_test = reviews_df_test.withColumn(\"sentiment_score\", sentiment_analyzer_udf(reviews_df_test[\"comments\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtSPIIMLqK0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fa5be72c-0111-4f90-b444-de769a47b09a"
      },
      "source": [
        "df.show(10,True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------+---------------+\n",
            "|                      comments|sentiment_score|\n",
            "+------------------------------+---------------+\n",
            "|          \tEvelyn Badia's l...|      0.4577972|\n",
            "|                   First of...|      0.2171875|\n",
            "|                  My girlfr...|     0.14798659|\n",
            "|                 I am a gra...|      0.3895009|\n",
            "|       像这样和房东住在一套...|            0.0|\n",
            "|                Santiago an...|      0.4224359|\n",
            "|                       so cute|            0.5|\n",
            "|               Allison is a...|          0.275|\n",
            "|               Cynthia was ...|     0.24724358|\n",
            "|               First off le...|     0.24210373|\n",
            "+------------------------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui6IG7RPqK0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Categorizing Sentences\n",
        "\n",
        "def sentiment(r): \n",
        "    if (r >= 0.1):\n",
        "        label = 1\n",
        "    elif(r <= 0):\n",
        "        label = 0\n",
        "    else:\n",
        "        label = r\n",
        "    return label\n",
        "\n",
        "sentiments_udf = udf(sentiment, IntegerType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3BcgFYEqK0L",
        "colab_type": "text"
      },
      "source": [
        "### Handling String Data:\n",
        "\n",
        "- Removing Non_Ascii\n",
        "- Checking Blanks\n",
        "- Fixing Abbreviations\n",
        "- Removing Stop Words\n",
        "- Lemmatizing Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSkDABA9w7tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "770848ba-03d4-4e4a-c6c4-d0b2e4e14130"
      },
      "source": [
        "!pip install langid"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from langid) (1.16.3)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/bc/61/50a93be85d1afe9436c3dc61f38da8ad7b637a38af4824e86e\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVehpqF6qK0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import udf, col\n",
        "from pyspark.sql.types import StringType, DoubleType, DateType\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "import langid\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufuV4XQvqK0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crating UDF To Strip Non_Ascii\n",
        "\n",
        "def strip_non_ascii(data_str):\n",
        "    ''' Returns the string without non ASCII characters''' \n",
        "    stripped = (c for c in data_str if 0 < ord(c) < 127) \n",
        "    return ''.join(stripped)\n",
        "\n",
        "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
        "\n",
        "df = df.withColumn('text_non_asci',strip_non_ascii_udf(df['comments']))\n",
        "df_test = df_test.withColumn('text_non_asci',strip_non_ascii_udf(df_test['comments']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wsEnbusqK0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "ecd0d6a5-137e-4e31-a5b1-d80be2ecdbf9"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+--------------------+\n",
            "|            comments|sentiment_score|       text_non_asci|\n",
            "+--------------------+---------------+--------------------+\n",
            "|\tEvelyn Badia's l...|      0.4577972|\tEvelyn Badia's l...|\n",
            "|        My girlfr...|     0.14798659|        My girlfr...|\n",
            "|       I am a gra...|      0.3895009|       I am a gra...|\n",
            "|       Robert  is...|     0.38421488|       Robert  is...|\n",
            "|             so cute|            0.5|             so cute|\n",
            "+--------------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK3hLc4JqK0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Crating UDF To Check Blanks\n",
        "\n",
        "def check_blanks(data_str):\n",
        "    is_blank = str(data_str.isspace()) \n",
        "    return is_blank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QK9YqdfqK0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "259ea4c3-372f-4c3f-e9e2-9fdfaf86e6b4"
      },
      "source": [
        "# Crating UDF To Fix Abbrs\n",
        "\n",
        "def fix_abbreviation(data_str):\n",
        "    data_str = data_str.lower()\n",
        "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str) \n",
        "    data_str = re.sub(r'\\bive\\b', 'i have', data_str) \n",
        "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
        "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
        "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str) \n",
        "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str) \n",
        "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str) \n",
        "    data_str = re.sub(r'\\bid\\b', 'i would', data_str) \n",
        "    data_str = re.sub(r'wtf', 'what the fuck', data_str) \n",
        "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str) \n",
        "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
        "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
        "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
        "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str) \n",
        "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
        "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str) \n",
        "    data_str = re.sub(r'rt\\b', '', data_str)\n",
        "    data_str = data_str.strip()\n",
        "    return data_str\n",
        "\n",
        "\n",
        "fix_abbr_udf = udf(fix_abbreviation, StringType()) \n",
        "\n",
        "df = df.withColumn('fix_abbr',fix_abbr_udf(df['text_non_asci']))\n",
        "df_test = df_test.withColumn('fix_abbr',fix_abbr_udf(df_test['text_non_asci']))\n",
        "df.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+--------------------+--------------------+\n",
            "|            comments|sentiment_score|       text_non_asci|            fix_abbr|\n",
            "+--------------------+---------------+--------------------+--------------------+\n",
            "|\tEvelyn Badia's l...|      0.4577972|\tEvelyn Badia's l...|evelyn badia's li...|\n",
            "|        My girlfr...|     0.14798659|        My girlfr...|my girlfriend and...|\n",
            "|       I am a gra...|      0.3895009|       I am a gra...|i am a graduate s...|\n",
            "|       Robert  is...|     0.38421488|       Robert  is...|robe  is really s...|\n",
            "|             so cute|            0.5|             so cute|             so cute|\n",
            "+--------------------+---------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-jcl5npxnxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "32841d1d-cb9c-4a6f-b964-f46471c0ae06"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q73vCYi2qK0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "6251348e-6387-4f48-daec-f734c2601999"
      },
      "source": [
        "# Crating UDF To Remove Stop Words\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "def remove_stops(data_str,stops=stops):\n",
        "    # expects a string\n",
        "\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    #data_str = data_str.strip()\n",
        "    text = data_str.split()\n",
        "    for word in text:\n",
        "        if word not in stops:\n",
        "            if list_pos == 0: \n",
        "                cleaned_str = word\n",
        "            else:\n",
        "                cleaned_str = cleaned_str + ' ' + word\n",
        "        list_pos += 1 \n",
        "    return cleaned_str\n",
        "\n",
        "remove_stops_udf = udf(remove_stops, StringType())\n",
        "\n",
        "\n",
        "df = df.withColumn('stop_text',remove_stops_udf(df['fix_abbr']))\n",
        "df_test = df_test.withColumn('stop_text',remove_stops_udf(df_test['fix_abbr']))\n",
        "\n",
        "df.show(5,True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+--------------------+--------------------+--------------------+\n",
            "|            comments|sentiment_score|       text_non_asci|            fix_abbr|           stop_text|\n",
            "+--------------------+---------------+--------------------+--------------------+--------------------+\n",
            "|\tEvelyn Badia's l...|      0.4577972|\tEvelyn Badia's l...|evelyn badia's li...|evelyn badia's li...|\n",
            "|        My girlfr...|     0.14798659|        My girlfr...|my girlfriend and...| girlfriend plann...|\n",
            "|       I am a gra...|      0.3895009|       I am a gra...|i am a graduate s...| graduate student...|\n",
            "|       Robert  is...|     0.38421488|       Robert  is...|robe  is really s...|robe really super...|\n",
            "|             so cute|            0.5|             so cute|             so cute|                cute|\n",
            "+--------------------+---------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujv53ntCqK0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "a73f1305-82de-493c-f1fb-6192754d4b6f"
      },
      "source": [
        "# Crating UDF To Remove Features\n",
        "\n",
        "def remove_features(data_str): # compile regex\n",
        "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?') \n",
        "    punc_re = re.compile('[%s]' % re.escape(string.punctuation)) \n",
        "    num_re = re.compile('(\\\\d+)')\n",
        "    mention_re = re.compile('@(\\w+)')\n",
        "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
        "    # convert to lowercase\n",
        "    data_str = data_str.lower()\n",
        "    # remove hyperlinks\n",
        "    data_str = url_re.sub(' ', data_str)\n",
        "    # remove @mentions\n",
        "    data_str = mention_re.sub(' ', data_str)\n",
        "    # remove puncuation\n",
        "    data_str = punc_re.sub(' ', data_str)\n",
        "    # remove numeric 'words'\n",
        "    data_str = num_re.sub(' ', data_str)\n",
        "    # remove non a-z 0-9 characters and words shorter than 1 characters \n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    for word in data_str.split():\n",
        "        if list_pos == 0:\n",
        "            if alpha_num_re.match(word) and len(word) > 1:\n",
        "                cleaned_str = word \n",
        "            else:\n",
        "                cleaned_str = ' '\n",
        "        else:\n",
        "            if alpha_num_re.match(word) and len(word) > 1:\n",
        "                cleaned_str = cleaned_str + ' ' + word \n",
        "            else:\n",
        "                cleaned_str += ' '\n",
        "        list_pos += 1\n",
        "    # remove unwanted space, *.split() will automatically split on \n",
        "    # whitespace and discard duplicates, the \" \".join() joins the \n",
        "    # resulting list into one string.\n",
        "    return \" \".join(cleaned_str.split())\n",
        "    # setup pyspark udf function\n",
        "    \n",
        "remove_features_udf = udf(remove_features, StringType())\n",
        "\n",
        "\n",
        "df = df.withColumn('removed',remove_features_udf(df['stop_text']))\n",
        "df_test = df_test.withColumn('removed',remove_features_udf(df_test['stop_text']))\n",
        "\n",
        "df.show(5,True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|            comments|sentiment_score|       text_non_asci|            fix_abbr|           stop_text|             removed|\n",
            "+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|\tEvelyn Badia's l...|      0.4577972|\tEvelyn Badia's l...|evelyn badia's li...|evelyn badia's li...|evelyn badia livi...|\n",
            "|        My girlfr...|     0.14798659|        My girlfr...|my girlfriend and...| girlfriend plann...|girlfriend planne...|\n",
            "|       I am a gra...|      0.3895009|       I am a gra...|i am a graduate s...| graduate student...|graduate student ...|\n",
            "|       Robert  is...|     0.38421488|       Robert  is...|robe  is really s...|robe really super...|robe really super...|\n",
            "|             so cute|            0.5|             so cute|             so cute|                cute|                cute|\n",
            "+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Srl9nZyoLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9e517f10-2401-4c59-f049-27396bd5da54"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeiREP9RqK0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d17b5807-a1d4-459e-b976-01e92364b457"
      },
      "source": [
        "# lemmatization\n",
        "def lemmatize(data_str):\n",
        "    # expects a string\n",
        "    list_pos = 0\n",
        "    cleaned_str = ''\n",
        "    lmtzr = WordNetLemmatizer() \n",
        "    text = data_str.split() \n",
        "    tagged_words = pos_tag(text) \n",
        "    for word in tagged_words:\n",
        "        if 'v' in word[1].lower():\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
        "        else:\n",
        "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
        "        if list_pos == 0: \n",
        "            cleaned_str = lemma\n",
        "        else:\n",
        "            cleaned_str = cleaned_str + ' ' + lemma\n",
        "        list_pos += 1 \n",
        "    return cleaned_str\n",
        "\n",
        "lemmatize_udf = udf(lemmatize, StringType())\n",
        "\n",
        "\n",
        "df = df.withColumn('lemmatized_text',lemmatize_udf(df['removed']))\n",
        "df_test = df_test.withColumn('lemmatized_text',lemmatize_udf(df_test['removed']))\n",
        "\n",
        "\n",
        "df.select(\"lemmatized_text\").show(5)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|     lemmatized_text|\n",
            "+--------------------+\n",
            "|evelyn badia livi...|\n",
            "|girlfriend plan v...|\n",
            "|graduate student ...|\n",
            "|robe really super...|\n",
            "|                cute|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7ljcRDqK0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking Only Sentiment Score and Lemmatized Text \n",
        "\n",
        "lemmat_text = df.select(\"sentiment_score\",\"lemmatized_text\")\n",
        "lemmat_text_test = df_test.select(\"sentiment_score\",\"lemmatized_text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9JK3LfqK0n",
        "colab_type": "text"
      },
      "source": [
        "### Model Building\n",
        "\n",
        "- Tokenizing\n",
        "- Hashing\n",
        "- IDF Matrix\n",
        "- Naive Bayes\n",
        "- Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ47_ZoqqK0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier \n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer \n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83GlSxLqK0r",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD87aqSjqK0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"lemmatized_text\", outputCol=\"words\")\n",
        "tokenized = tokenizer.transform(lemmat_text)\n",
        "tokenized_test = tokenizer.transform(lemmat_text_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-fE_ADOqK0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "fa6928e2-5677-4024-b42b-ed4559a93a1d"
      },
      "source": [
        "tokenized_test.show(2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------------+--------------------+\n",
            "|sentiment_score|     lemmatized_text|               words|\n",
            "+---------------+--------------------+--------------------+\n",
            "|     0.38333333|ouii great hostsh...|[ouii, great, hos...|\n",
            "|      0.2171875|first all angelo ...|[first, all, ange...|\n",
            "+---------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0tKQ8hEqK0y",
        "colab_type": "text"
      },
      "source": [
        "#### Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXyDSTz-qK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hashing the tokenized document\n",
        "hashingTF = HashingTF (inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "hashtf = hashingTF.transform(tokenized)\n",
        "hashtf_test = hashingTF.transform(tokenized_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC0t7EO2qK01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ccd46bed-3584-4546-8f95-9063e1cd4ad6"
      },
      "source": [
        "hashtf.show(2)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------------+--------------------+--------------------+\n",
            "|sentiment_score|     lemmatized_text|               words|         rawFeatures|\n",
            "+---------------+--------------------+--------------------+--------------------+\n",
            "|      0.4577972|evelyn badia livi...|[evelyn, badia, l...|(262144,[6183,736...|\n",
            "|      0.2171875|first angelo grea...|[first, angelo, g...|(262144,[14,3067,...|\n",
            "+---------------+--------------------+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTrn81i6qK04",
        "colab_type": "text"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTtBiONzqK04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Creating TF-IDF Matrix\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=3)\n",
        "idf = idf.fit(hashtf)\n",
        "tfidf = idf.transform(hashtf)\n",
        "tfidf_test = idf.transform(hashtf_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZuIalgkqK06",
        "colab_type": "code",
        "colab": {},
        "outputId": "d9604e3a-16e5-4f6c-8c73-fd9ceba27598"
      },
      "source": [
        "tfidf.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXTtfm8rqK09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d3d8361f-2f43-40c4-b0d9-90578bc18866"
      },
      "source": [
        "tfidf.show(5)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|sentiment_score|     lemmatized_text|               words|         rawFeatures|            features|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|      0.4577972|evelyn badia livi...|[evelyn, badia, l...|(262144,[6183,736...|(262144,[6183,736...|\n",
            "|      0.2171875|first angelo grea...|[first, angelo, g...|(262144,[14,3067,...|(262144,[14,3067,...|\n",
            "|     0.14798659|girlfriend plan v...|[girlfriend, plan...|(262144,[14,991,1...|(262144,[14,991,1...|\n",
            "|      0.3895009|graduate student ...|[graduate, studen...|(262144,[11018,13...|(262144,[11018,13...|\n",
            "|            0.0|carlos carlos car...|[carlos, carlos, ...|(262144,[251381],...|(262144,[251381],...|\n",
            "+---------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOu94u6vqK0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selecting reuqired columns\n",
        "\n",
        "tfidf_clean = tfidf.withColumn(\"label\",sentiments_udf(tfidf[\"sentiment_score\"]))\n",
        "tfidf_clean_test = tfidf_test.withColumn(\"label\",sentiments_udf(tfidf_test[\"sentiment_score\"]))\n",
        "\n",
        "tfidf_clean= tfidf_clean.select(\"features\",\"label\")\n",
        "tfidf_clean_test= tfidf_clean_test.select(\"features\",\"label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhrMzrvWqK1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "995d1790-5294-454c-81f0-391d0aa47413"
      },
      "source": [
        "tfidf_clean.show(3)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262144,[6183,736...|    1|\n",
            "|(262144,[14,3067,...|    1|\n",
            "|(262144,[14,991,1...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-FNQWXuzqK1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c999903e-5387-4b22-913e-d8769b987e60"
      },
      "source": [
        "tfidf_clean=tfidf_clean.filter(col(\"label\").isin([1,0]))\n",
        "tfidf_clean_test=tfidf_clean_test.filter(col(\"label\").isin([1,0]))\n",
        "\n",
        "tfidf_clean.show(2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262144,[6183,736...|    1|\n",
            "|(262144,[14,991,1...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fimb9JlKqK1J",
        "colab_type": "text"
      },
      "source": [
        "### Fitting Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNUAZOdQqK1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Classifier\n",
        "nb = NaiveBayes(modelType=\"multinomial\")\n",
        "\n",
        "# Fittinf on Train Data\n",
        "nb_fit=nb.fit(tfidf_clean)\n",
        "\n",
        "# Predictions on Train \n",
        "predictions = nb_fit.transform(tfidf_clean)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EgmsYNqqK1S",
        "colab_type": "code",
        "colab": {},
        "outputId": "12dace15-4b50-45f1-cfe4-56246146f661"
      },
      "source": [
        "# Evaluating on Train Set\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\") \n",
        "accuracy_nb_train = evaluator.evaluate(predictions)\n",
        "accuracy_nb_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9484898504777135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5pZCPrqK1V",
        "colab_type": "code",
        "colab": {},
        "outputId": "42adf176-8342-4a5e-90a9-76399c88c304"
      },
      "source": [
        "# Prediction on test\n",
        "predictions_test = nb_fit.transform(tfidf_clean_test)\n",
        "\n",
        "# Accuracy Evaluation test\n",
        "accuracy_nb_test = evaluator.evaluate(predictions_test)\n",
        "accuracy_nb_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9471685247710722"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrRpU1-BqK1Y",
        "colab_type": "text"
      },
      "source": [
        "### Fitting Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJQa03CuWM7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8O9ZFg4WM7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_fit=dt.fit(tfidf_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FPHyRR1d8SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_dt_train = dt_fit.transform(tfidf_clean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owt-B2rMWM7m",
        "colab_type": "code",
        "outputId": "86a7f57d-33cb-4f07-f474-1bf47182de48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\") \n",
        "evaluator.evaluate(predictions_dt_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9564995456968696"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNOJh4-WM7n",
        "colab_type": "code",
        "outputId": "176eb720-91a4-415b-d5db-b03f21e5fa1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predictions_dt_test = dt_fit.transform(tfidf_clean_test)\n",
        "accuracy_dt_test = evaluator.evaluate(predictions_dt_test)\n",
        "accuracy_dt_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9515064939642668"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    }
  ]
}